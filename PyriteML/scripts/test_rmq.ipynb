{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "\n",
    "SCRIPT_PATH = \"/home/yifanhou/git/PyriteML/scripts\"\n",
    "sys.path.append(os.path.join(SCRIPT_PATH, '../'))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import dill\n",
    "import hydra\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace\n",
    "from diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset\n",
    "from diffusion_policy.workspace.train_diffusion_unet_image_workspace import TrainDiffusionUnetImageWorkspace\n",
    "\n",
    "data_path = \"/home/yifanhou/training_outputs/\"\n",
    "ckpt_path = data_path + \"2025.03.05_21.53.36_stow_no_force_202_stow_80/checkpoints/latest.ckpt\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# load checkpoint\n",
    "if not ckpt_path.endswith('.ckpt'):\n",
    "    ckpt_path = os.path.join(ckpt_path, 'checkpoints', 'latest.ckpt')\n",
    "payload = torch.load(open(ckpt_path, 'rb'), map_location='cpu', pickle_module=dill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robotmq as rmq\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "server = rmq.RMQServer(\n",
    "    server_name=\"test_rmq_server\", server_endpoint=\"ipc:///tmp/feeds/0\"\n",
    ")\n",
    "client = rmq.RMQClient(\n",
    "    client_name=\"test_rmq_client\", server_endpoint=\"ipc:///tmp/feeds/0\"\n",
    ")\n",
    "print(\"Server and client created\")\n",
    "\n",
    "server.add_topic(\"test_checkpoints\", 10)\n",
    "\n",
    "# Serialize the checkpoint\n",
    "start_time = time.time()\n",
    "pickle_data = pickle.dumps(payload)\n",
    "dump_end_time = time.time()\n",
    "server.put_data(\"test_checkpoints\", pickle_data)\n",
    "send_end_time = time.time()\n",
    "time.sleep(0.01)\n",
    "\n",
    "retrieve_start_time = time.time()\n",
    "retrieved_data, timestamp = client.peek_data(topic=\"test_checkpoints\", order=\"latest\", n=1)\n",
    "retrieve_end_time = time.time()\n",
    "received_data = pickle.loads(retrieved_data[0])\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Data size: {len(pickle_data) / 1024**2:.3f}MB. dump: {dump_end_time - start_time:.4f}s, send: {send_end_time - dump_end_time: .4f}s, retrieve: {retrieve_end_time - retrieve_start_time:.4f}s, load: {time.time() - retrieve_end_time:.4f}s)\"\n",
    ")\n",
    "\n",
    "\n",
    "# use the received payload\n",
    "cfg = received_data['cfg']\n",
    "print(\"dataset_path:\", cfg.task.dataset.dataset_path)\n",
    "\n",
    "cls = hydra.utils.get_class(cfg._target_)\n",
    "workspace = cls(cfg)\n",
    "workspace: BaseWorkspace\n",
    "workspace.load_payload(received_data, exclude_keys=None, include_keys=None)\n",
    "\n",
    "policy = workspace.model\n",
    "if cfg.training.use_ema:\n",
    "    policy = workspace.ema_model\n",
    "policy.num_inference_steps = cfg.policy.num_inference_steps # DDIM inference iterations\n",
    "\n",
    "policy.eval().to(device)\n",
    "policy.reset()\n",
    "\n",
    "# use normalizer saved in the policy\n",
    "sparse_normalizer, dense_normalizer = policy.get_normalizer()\n",
    "\n",
    "shape_meta = cfg.task.shape_meta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
